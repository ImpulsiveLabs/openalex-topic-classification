{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7676996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b75a71-0851-4f1c-8bbd-aead19ebac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5523bfb-7a83-4508-8f2a-de3eccdbe149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import evaluate\n",
    "from datasets import load_dataset, load_metric, list_metrics\n",
    "from transformers import create_optimizer\n",
    "from transformers import create_optimizer, TFAutoModelForSequenceClassification, DistilBertTokenizer\n",
    "from transformers import DataCollatorWithPadding, TFDistilBertForSequenceClassification\n",
    "from transformers import TFRobertaForSequenceClassification, RobertaTokenizer, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e47842-43c7-42cb-b54c-5d1393463c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf19aa8-a913-454f-ab5a-ec596382f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_citation_feature_for_model(list_of_links, num_to_take):\n",
    "    if len(list_of_links) <= num_to_take:\n",
    "        return list_of_links\n",
    "    else:\n",
    "        return random.sample(list_of_links, num_to_take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "272eb837-fb87-4dca-b470-5595d82d3c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(embedding):\n",
    "    \n",
    "    if isinstance(embedding, list):\n",
    "        return np.array(embedding, dtype=np.float32)\n",
    "    else:\n",
    "        return np.zeros(384, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6f2964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_abstract_to_abstract(invert_abstract):\n",
    "    invert_abstract = json.loads(invert_abstract)\n",
    "    ab_len = invert_abstract['IndexLength']\n",
    "    \n",
    "    if 30 < ab_len < 1000:\n",
    "        abstract = [\" \"]*ab_len\n",
    "        for key, value in invert_abstract['InvertedIndex'].items():\n",
    "            for i in value:\n",
    "                abstract[i] = key\n",
    "        final_abstract = \" \".join(abstract)\n",
    "    else:\n",
    "        final_abstract = None\n",
    "    return final_abstract\n",
    "\n",
    "def clean_abstract(abstract, inverted=True):\n",
    "    if inverted:\n",
    "        if abstract:\n",
    "            abstract = invert_abstract_to_abstract(abstract)\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        if isinstance(abstract, str):\n",
    "            ab_len = len(abstract)\n",
    "            if ab_len < 30:\n",
    "                abstract = None\n",
    "#     abstract = clean_text(abstract)\n",
    "    return abstract\n",
    "\n",
    "def clean_text(text):\n",
    "    try:\n",
    "        text = text.lower()\n",
    "\n",
    "        text = re.sub('[^a-zA-Z0-9 ]+', ' ', text)\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "    except:\n",
    "        text = \"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e997284-5c48-4b5a-93a2-eb43ab6b6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_title_and_abstract(title, abstract):\n",
    "    if isinstance(title, str):\n",
    "        if isinstance(abstract, str):\n",
    "            return f\"<TITLE> {title}\\n<ABSTRACT> {abstract}\"\n",
    "        else:\n",
    "            return f\"<TITLE> {title}\"\n",
    "    else:\n",
    "        if isinstance(abstract, str):\n",
    "            return f\"<TITLE> NONE\\n<ABSTRACT> {abstract}\"\n",
    "        else:\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9940021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(df, column, starting_num = 0):\n",
    "    # Create a vocab out of the column\n",
    "    vocab = df[column].unique()\n",
    "\n",
    "    # Create a dict that maps vocab to integers\n",
    "    vocab_to_int = {word: i+starting_num for i, word in enumerate(vocab)}\n",
    "    \n",
    "    inv_vocab_to_int = {i:j for j,i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, inv_vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e0fac0-d2d7-434f-b8d6-e5a414861c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_citation_feature(citations, emb_vocab, gold_to_label_mapping, num_to_keep):\n",
    "    if citations:\n",
    "        mapped_cites = [gold_to_label_mapping.get(x) for x in citations if gold_to_label_mapping.get(x)]\n",
    "        temp_feature = [emb_vocab[x] for x in mapped_cites]\n",
    "    \n",
    "        if len(temp_feature) < num_to_keep:\n",
    "            return temp_feature + [0]*(num_to_keep - len(temp_feature))\n",
    "        else:\n",
    "            return temp_feature\n",
    "    else:\n",
    "        return [1] + [0]*(num_to_keep - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b2760cb-65d5-4051-b301-d3f2eb8cab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_string_of_list(list_to_sort):\n",
    "    list_to_sort.sort()\n",
    "    return \"|\".join([str(x) for x in list_to_sort])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "792a14b0-915f-4fb4-ba7f-f2f81d9b6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle(pickle_path):\n",
    "    # Open the pickle file\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        pickle_dict = pickle.load(f)\n",
    "\n",
    "    return pickle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eea36bd6-1574-4817-b293-0e64ac464b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(dictionary, file_path):\n",
    "    # Save the dictionary as a pickle file\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e3f6d-0feb-42ec-a40c-6a4dba9d0a07",
   "metadata": {},
   "source": [
    "#### Getting Gold Citations Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d607c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126773, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_to_id_mapping = pd.read_parquet(\"s3://data-pull-from-justins-personal-s3/all_concepts_model_data/V4/citation_model_new_less_gold/gold_citation_papers_single_file/part-00000-tid-3822763855470313237-f26ac12e-de75-40aa-84fb-9fc177530185-12895-1-c000.snappy.parquet\")\n",
    "gold_to_id_mapping.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "006b53e1-85b8-468b-a71a-ee118ac89fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124577, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_citation_grouped = gold_to_id_mapping.groupby('gold_citation')['micro_cluster_id'].apply(list).reset_index()\n",
    "gold_citation_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1967c1f6-a3e6-48d2-ac20-552e328b3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_citation_grouped['cluster_string'] = gold_citation_grouped['micro_cluster_id'].apply(get_sorted_string_of_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9d41b06-72d0-485e-96d0-cddced12f4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_citation</th>\n",
       "      <th>micro_cluster_id</th>\n",
       "      <th>cluster_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86397</th>\n",
       "      <td>2149443601</td>\n",
       "      <td>[1667]</td>\n",
       "      <td>1667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28642</th>\n",
       "      <td>1999013095</td>\n",
       "      <td>[2315]</td>\n",
       "      <td>2315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       gold_citation micro_cluster_id cluster_string\n",
       "86397     2149443601           [1667]           1667\n",
       "28642     1999013095           [2315]           2315"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_citation_grouped.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "180a04a9-01b9-4b33-8de2-d3e71fede828",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_to_id_mapping_dict = {x:y for x,y in zip(gold_citation_grouped['gold_citation'].tolist(), \n",
    "                                              gold_citation_grouped['cluster_string'].tolist())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b20db5-4ae6-4bfb-b6cf-07e59cb22774",
   "metadata": {},
   "source": [
    "#### Processing All Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61ae8766-7f1d-4d15-8323-dc5c91ba5d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-multilingual-cased\"\n",
    "task = \"openalex-topic-classification-title-abstract\"\n",
    "language_model_name = f\"OpenAlex/{model_name}-finetuned-{task}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd16f2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4521000, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_parquet(\"{path_to_all_data_from_004_spark_file}\", \n",
    "                           columns=['paper_id','new_title','abstract','final_level_0_links',\n",
    "                                     'final_level_1_links','journal_id','micro_cluster_id','short_label',\n",
    "                                     'long_label'])\n",
    "\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d804807",
   "metadata": {},
   "source": [
    "##### Process title and abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cd9d2c3-52e2-44b9-b736-80a2e0aa212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['new_title'] = all_data['new_title'].apply(lambda x: None if x=='' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27095822",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['abstract_processed'] = all_data['abstract'].apply(lambda x: clean_abstract(x, inverted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "548a3259-b2d3-42d8-a4f6-e965e70644d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['title_abstract'] = all_data.apply(lambda x: merge_title_and_abstract(x.new_title, \n",
    "                                                                                                 x.abstract_processed), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7172abd9",
   "metadata": {},
   "source": [
    "##### Process label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "773c69e9-571f-45b4-8c16-f8011646c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['full_label'] = all_data.apply(lambda x: f\"{x.micro_cluster_id}: {x.long_label}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df335f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = all_data[['micro_cluster_id','long_label']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fdf58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab, inv_target_vocab = create_vocab(all_data, 'full_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['label'] = all_data['full_label'].apply(lambda x: target_vocab[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2ce63",
   "metadata": {},
   "source": [
    "##### Process citation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a9feaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['level_0_links_feature'] = all_data['final_level_0_links'].apply(lambda x: get_final_citation_feature_for_model(x.tolist(), 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34dfae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['level_1_links_feature'] = all_data['final_level_1_links'].apply(lambda x: get_final_citation_feature_for_model(x.tolist(), 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63cf43c2-df99-488b-ae18-89163e30bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_feature_vocab, inv_citation_feature_vocab = create_vocab(gold_citation_grouped, 'cluster_string', starting_num = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f79ad308-989f-4bd4-a7b7-27d34b2ca60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['level_0_citation'] = all_data.apply(lambda x :transform_citation_feature(x.level_0_links_feature, citation_feature_vocab, \n",
    "                                                                                   gold_to_id_mapping_dict, 16), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d40f6cd-e105-4fcb-9872-721cb221cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['level_1_citation'] = all_data.apply(lambda x :transform_citation_feature(x.level_1_links_feature, citation_feature_vocab, \n",
    "                                                                                   gold_to_id_mapping_dict, 128), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091637b",
   "metadata": {},
   "source": [
    "##### Save data and artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fd7d28e-0be1-4e15-b43f-c59f89108a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[['paper_id','full_label','title_abstract','level_0_citation',\n",
    "               'level_1_citation','label','journal_id']]\\\n",
    "    .to_parquet(\"./model_artifacts/all_data_temp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67249ffe-bde3-4e30-bbe9-5ebc5032d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = save_pickle(target_vocab, './model_artifacts/target_vocab.pkl')\n",
    "_ = save_pickle(inv_target_vocab , './model_artifacts/inv_target_vocab.pkl')\n",
    "_ = save_pickle(citation_feature_vocab, './model_artifacts/citation_feature_vocab.pkl')\n",
    "_ = save_pickle(inv_citation_feature_vocab , './model_artifacts/inv_citation_feature_vocab.pkl')\n",
    "_ = save_pickle(gold_to_id_mapping_dict , './model_artifacts/gold_to_id_mapping_dict.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d0c19",
   "metadata": {},
   "source": [
    "##### Tokenize title and abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2660e647-1373-4cc5-860c-1b7a7acb6056",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_parquet(\"./model_artifacts/all_data_temp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa9a0f8-3061-463b-b348-b0e5a47b2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(language_model_name, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96380575-8714-44be-a0c5-b05f651e0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_abs_tok = tokenizer(all_data['title_abstract'].tolist(), max_length=512, truncation=True, padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b03c41dd-8a68-4ccc-ba3c-40a67a7ce35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['input_ids'] = [np.array(x) for x in title_abs_tok['input_ids']]\n",
    "all_data['attention_mask'] = [np.array(x) for x in title_abs_tok['attention_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd137e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[['paper_id','full_label','title_abstract','level_0_citation',\n",
    "               'level_1_citation','label','input_ids','attention_mask','journal_id']]\\\n",
    "    .to_parquet(\"./model_artifacts/all_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba7485",
   "metadata": {},
   "source": [
    "#### Processing journal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_journal_emb(journal_name):\n",
    "    if check_for_non_latin_characters(journal_name) == 1:\n",
    "        return emb_model.encode(str(journal_name))\n",
    "    else:\n",
    "        return np.zeros(384, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "524bdb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_data = pd.read_parquet(\"./model_artifacts/all_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fce3008-63d8-411a-8c60-9d4af1bb0b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_data['journal_id'] = all_processed_data['journal_id'].fillna(-1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b56cd835-96e1-4aa7-ac39-dc550f8a592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These journal embeddings were created using the 005 file in the data preprocessing folder\n",
    "journal_embs = open_pickle('./journal_embs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1630afd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4521000, 9)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_data = all_processed_data.sample(all_processed_data.shape[0], random_state=0)\n",
    "shuffled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03f87492-a83a-476b-91a4-fefd872b2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data['journal_emb'] = shuffled_data['journal_id'].apply(lambda x: journal_embs.get(x, np.zeros(384, dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bffda30",
   "metadata": {},
   "source": [
    "##### Creating random even splits of the training data (so labeled are balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2d42c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data['row_num'] = shuffled_data.groupby('label').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2eb5b2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4294950, 11)\n",
      "(113025, 11)\n",
      "(113025, 11)\n"
     ]
    }
   ],
   "source": [
    "train = shuffled_data[shuffled_data['row_num']<=950].copy()\n",
    "val = shuffled_data[(shuffled_data['row_num']>950) & (shuffled_data['row_num']<=975)].copy()\n",
    "test = shuffled_data[shuffled_data['row_num']>975].copy()\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "407a6243-8e37-47a9-b58a-e6dc91cbd672",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(\"./model_artifacts/train.parquet\")\n",
    "val.to_parquet(\"./model_artifacts/val.parquet\")\n",
    "test.to_parquet(\"./model_artifacts/test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb21b0-656a-4785-b228-c062e01c485d",
   "metadata": {},
   "source": [
    "#### Load Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a09f68fb-8931-456c-a80a-d917fb21406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes, emb_table_size):\n",
    "    # Load finetuned language model\n",
    "    model_name = \"bert-base-multilingual-cased\"\n",
    "    task = \"openalex-topic-classification-title-abstract\"\n",
    "    language_model_name = f\"OpenAlex/{model_name}-finetuned-{task}\"\n",
    "    language_model = TFAutoModelForSequenceClassification.from_pretrained(language_model_name, \n",
    "                                                                          output_hidden_states=True)\n",
    "    language_model.trainable = False\n",
    "\n",
    "\n",
    "    # Inputs\n",
    "    ids = tf.keras.layers.Input((512,), dtype=tf.int64, name='ids')\n",
    "    mask = tf.keras.layers.Input((512,), dtype=tf.int64, name='mask')\n",
    "    citation_0 = tf.keras.layers.Input((16,), dtype=tf.int64, name='citation_0')\n",
    "    citation_1 = tf.keras.layers.Input((128,), dtype=tf.int64, name='citation_1')\n",
    "    journal = tf.keras.layers.Input((384,), dtype=tf.float32, name='journal_emb')\n",
    "    \n",
    "    language_model_output = language_model(input_ids=ids, attention_mask=mask).hidden_states[-1]\n",
    "    pooled_language_model_output = tf.keras.layers.GlobalAveragePooling1D()(language_model_output)\n",
    "    \n",
    "    citation_emb_layer = tf.keras.layers.Embedding(input_dim=emb_table_size, output_dim=256, mask_zero=True, \n",
    "                                                   trainable=True, name='citation_emb_layer')\n",
    "\n",
    "    citation_0_emb = citation_emb_layer(citation_0)\n",
    "    citation_1_emb = citation_emb_layer(citation_1)\n",
    "\n",
    "    pooled_citation_0 = tf.keras.layers.GlobalAveragePooling1D()(citation_0_emb)\n",
    "    pooled_citation_1 = tf.keras.layers.GlobalAveragePooling1D()(citation_1_emb)\n",
    "\n",
    "    concat_data = tf.keras.layers.Concatenate(name='concat_data', axis=-1)([pooled_language_model_output, \n",
    "                                                                            pooled_citation_0, \n",
    "                                                                            pooled_citation_1, journal])\n",
    "\n",
    "    # Dense layer 1\n",
    "    dense_output = tf.keras.layers.Dense(2048, activation='relu', \n",
    "                                         kernel_regularizer='L2', name=\"dense_1\")(concat_data)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_1\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_1\")(dense_output)\n",
    "    \n",
    "    # Dense layer 2\n",
    "    dense_output = tf.keras.layers.Dense(1024, activation='relu', \n",
    "                                         kernel_regularizer='L2', name=\"dense_2\")(dense_output)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_2\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_2\")(dense_output)\n",
    "\n",
    "    # Dense layer 3\n",
    "    dense_output = tf.keras.layers.Dense(512, activation='relu', \n",
    "                                         kernel_regularizer='L2', name=\"dense_3\")(dense_output)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_3\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_3\")(dense_output)\n",
    "\n",
    "    class_prior = 1/len(target_vocab)\n",
    "    last_layer_weight_init = tf.keras.initializers.Constant(class_prior)\n",
    "    last_layer_bias_init = tf.keras.initializers.Constant(-np.log((1-class_prior)/class_prior))\n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(num_classes, kernel_initializer=last_layer_weight_init,\n",
    "                                         bias_initializer=last_layer_bias_init,\n",
    "                                         activation='sigmoid', name='output_layer')(dense_output)\n",
    "    model = tf.keras.Model(inputs=[ids, mask, citation_0, citation_1, journal], outputs=output_layer)\n",
    "\n",
    "    loss_fn = tf.keras.losses.CategoricalFocalCrossentropy()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.AdamW(), \n",
    "                  loss=loss_fn,\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy(), \n",
    "                           tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top_2_categorical_accuracy'),\n",
    "                           tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_categorical_accuracy'),\n",
    "                           tf.keras.metrics.TopKCategoricalAccuracy(k=10, name='top_10_categorical_accuracy')])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc68d18e-cb6e-42f0-b29f-ed563595bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = open_pickle('./model_artifacts/target_vocab.pkl')\n",
    "inv_target_vocab = open_pickle('./model_artifacts/inv_target_vocab.pkl')\n",
    "citation_feature_vocab = open_pickle('./model_artifacts/citation_feature_vocab.pkl')\n",
    "inv_citation_feature_vocab = open_pickle('./model_artifacts/inv_citation_feature_vocab.pkl')\n",
    "gold_to_id_mapping_dict = open_pickle('./model_artifacts/gold_to_id_mapping_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51938e10-6f0d-4b3a-9a02-b3af6d5a898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels_and_data(examples):\n",
    "    \"\"\"\n",
    "    This function was used to introduce \"missing\" data into the training data. Because the data we are\n",
    "    training on is not representative of the data seen in production, we need to teach the model how to\n",
    "    make predictions when not all of the data is available.\n",
    "    \"\"\"\n",
    "    examples['label'] = tf.keras.utils.to_categorical(examples['label'], num_classes=len(target_vocab))\n",
    "    if (examples['citation_0'][0]!=1) | (examples['citation_1'][0]!=1):\n",
    "        rand_num = random.random()\n",
    "        if rand_num < 0.15:\n",
    "            examples['ids'] = [101, 102] + [0]*510\n",
    "            examples['mask'] = [1, 1] + [0]*510\n",
    "\n",
    "        rand_num = random.random()\n",
    "        if rand_num < 0.25:\n",
    "            examples['journal_emb'] = [0.0]*384\n",
    "\n",
    "    if (examples['citation_0'][0]==1) & (examples['citation_1'][0]==1):\n",
    "        if np.mean(examples['journal_emb']) != 0.0:\n",
    "            rand_num = random.random()\n",
    "            if rand_num < 0.10:\n",
    "                examples['ids'] = [101, 102] + [0]*510\n",
    "                examples['mask'] = [1, 1] + [0]*510\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9a137f0-6a49-442a-a0af-3cb3b6cd026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels(examples):\n",
    "    examples['label'] = tf.keras.utils.to_categorical(examples['label'], num_classes=len(target_vocab))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c216812-da92-434c-914c-a2f0b6262b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the HuggingFace library to load the dataset\n",
    "all_dataset = load_dataset(\"parquet\", data_files={'train': [\"./model_artifacts/train.parquet\"]}) \\\n",
    "    .rename_column(\"level_0_citation\", \"citation_0\") \\\n",
    "    .rename_column(\"level_1_citation\", \"citation_1\") \\\n",
    "    .rename_column(\"input_ids\", \"ids\") \\\n",
    "    .rename_column(\"attention_mask\", \"mask\").map(preprocess_labels_and_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96cf70d9-891f-4a09-8ad9-bb8e35a01319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the HuggingFace library to load the dataset\n",
    "all_dataset_val = load_dataset(\"parquet\", data_files={'val': [\"./model_artifacts/val.parquet\"]}) \\\n",
    "    .rename_column(\"level_0_citation\", \"citation_0\") \\\n",
    "    .rename_column(\"level_1_citation\", \"citation_1\") \\\n",
    "    .rename_column(\"input_ids\", \"ids\") \\\n",
    "    .rename_column(\"attention_mask\", \"mask\").map(preprocess_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fc1fb-4d1d-4bc9-b5ca-8c03992ed67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow for use of multiple GPUs\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "batch_size = 2048\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    concept_model = create_model(len(target_vocab), len(citation_feature_vocab)+2)\n",
    "\n",
    "    tf_train_dataset = all_dataset['train'].to_tf_dataset(\n",
    "        columns=['ids','mask','citation_0','citation_1','journal_emb'],\n",
    "        label_cols=[\"label\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    tf_val_dataset = all_dataset_val['val'].to_tf_dataset(\n",
    "        columns=['ids','mask','citation_0','citation_1','journal_emb'],\n",
    "        label_cols=[\"label\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da96f83d-1147-4c75-9907-9f37c517ee5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " ids (InputLayer)            [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " mask (InputLayer)           [(None, 512)]                0         []                            \n",
      "                                                                                                  \n",
      " citation_0 (InputLayer)     [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " citation_1 (InputLayer)     [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " tf_bert_for_sequence_class  TFSequenceClassifierOutput   1813300   ['ids[0][0]',                 \n",
      " ification_1 (TFBertForSequ  (loss=None, logits=(None,    89         'mask[0][0]']                \n",
      " enceClassification)         4521),                                                               \n",
      "                              hidden_states=((None, 512                                           \n",
      "                             , 768),                                                              \n",
      "                              (None, 512, 768),                                                   \n",
      "                              (None, 512, 768),                                                   \n",
      "                              (None, 512, 768),                                                   \n",
      "                              (None, 512, 768),                                                   \n",
      "                              (None, 512, 768),                                                   \n",
      "                              (None, 512, 768),                                                   \n",
      "                              (None, 512, 768),                                                   \n",
      "                              (None, 512, 768),                                                   \n",
      "                              (None, 512, 768),                                                   \n",
      "                              (None, 512, 768),                                                   \n",
      "                              (None, 512, 768),                                                   \n",
      "                              (None, 512, 768)),                                                  \n",
      "                              attentions=None)                                                    \n",
      "                                                                                                  \n",
      " citation_emb_layer (Embedd  multiple                     1538048   ['citation_0[0][0]',          \n",
      " ing)                                                                'citation_1[0][0]']          \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 768)                  0         ['tf_bert_for_sequence_classif\n",
      "  (GlobalAveragePooling1D)                                          ication_1[0][12]']            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4  (None, 256)                  0         ['citation_emb_layer[0][0]']  \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " global_average_pooling1d_5  (None, 256)                  0         ['citation_emb_layer[1][0]']  \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " journal_emb (InputLayer)    [(None, 384)]                0         []                            \n",
      "                                                                                                  \n",
      " concat_data (Concatenate)   (None, 1664)                 0         ['global_average_pooling1d_3[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'global_average_pooling1d_4[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'global_average_pooling1d_5[0\n",
      "                                                                    ][0]',                        \n",
      "                                                                     'journal_emb[0][0]']         \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 2048)                 3409920   ['concat_data[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 2048)                 0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " layer_norm_1 (LayerNormali  (None, 2048)                 4096      ['dropout_1[0][0]']           \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1024)                 2098176   ['layer_norm_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 1024)                 0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " layer_norm_2 (LayerNormali  (None, 1024)                 2048      ['dropout_2[0][0]']           \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 512)                  524800    ['layer_norm_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 512)                  0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " layer_norm_3 (LayerNormali  (None, 512)                  1024      ['dropout_3[0][0]']           \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " output_layer (Dense)        (None, 4521)                 2319273   ['layer_norm_3[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 191227474 (729.47 MB)\n",
      "Trainable params: 9897385 (37.76 MB)\n",
      "Non-trainable params: 181330089 (691.72 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "concept_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9fdfe-1f4a-4c09-b11f-748632f5e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/{epoch:02d}-{val_loss:.3f}-{val_categorical_accuracy:.4f}-{val_top_5_categorical_accuracy:.4f}.keras',\n",
    "        save_weights_only=False,\n",
    "        save_best_only=False)\n",
    "\n",
    "callbacks = [early_stopping, model_checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674c4b4-fab1-468d-bbd0-9d9100c1b1fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = concept_model.fit(tf_train_dataset, epochs=20, validation_data=tf_val_dataset, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be39036-1db3-4f80-be8d-dce07b9661df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
