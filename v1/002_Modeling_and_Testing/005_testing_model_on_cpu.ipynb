{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b4df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import unicodedata\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39e66e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca187a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 16:39:42.381705: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-26 16:39:42.430216: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-26 16:39:43.196971: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification, pipeline, AutoTokenizer, TextClassificationPipeline, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87fd1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_keep_ind(groups):\n",
    "    # Groups of characters that do not perform well\n",
    "    groups_to_skip = ['HIRAGANA', 'CJK', 'KATAKANA','ARABIC', 'HANGUL', 'THAI','DEVANAGARI','BENGALI',\n",
    "                      'THAANA','GUJARATI','CYRILLIC']\n",
    "    \n",
    "    if any(x in groups_to_skip for x in groups):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def remove_non_latin_characters(text):\n",
    "    final_char = []\n",
    "    groups_to_skip = ['HIRAGANA', 'CJK', 'KATAKANA','ARABIC', 'HANGUL', 'THAI','DEVANAGARI','BENGALI',\n",
    "                      'THAANA','GUJARATI','CYRILLIC']\n",
    "    for char in text:\n",
    "        try:\n",
    "            script = unicodedata.name(char).split(\" \")[0]\n",
    "            if script not in groups_to_skip:\n",
    "                final_char.append(char)\n",
    "        except:\n",
    "            pass\n",
    "    return \"\".join(final_char)\n",
    "    \n",
    "def group_non_latin_characters(text):\n",
    "    groups = []\n",
    "    latin_chars = []\n",
    "    text = text.replace(\".\", \"\").replace(\" \", \"\")\n",
    "    for char in text:\n",
    "        try:\n",
    "            script = unicodedata.name(char).split(\" \")[0]\n",
    "            if script == 'LATIN':\n",
    "                latin_chars.append(script)\n",
    "            else:\n",
    "                if script not in groups:\n",
    "                    groups.append(script)\n",
    "        except:\n",
    "            if \"UNK\" not in groups:\n",
    "                groups.append(\"UNK\")\n",
    "    return groups, len(latin_chars)\n",
    "\n",
    "def check_for_non_latin_characters(text):\n",
    "    groups, latin_chars = group_non_latin_characters(str(text))\n",
    "    if name_to_keep_ind(groups) == 1:\n",
    "        return 1\n",
    "    elif latin_chars > 20:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cfa986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_journal_emb(journal_name):\n",
    "    # Strip white space\n",
    "    if isinstance(journal_name, str):\n",
    "        journal_name = journal_name.strip()\n",
    "\n",
    "        # Removing all journal names with eBook (most are not descriptive)\n",
    "        if 'eBooks' in journal_name:\n",
    "            return np.zeros(384, dtype=np.float32)\n",
    "\n",
    "        # Check if non-latin characters are dominant (embedding model not good for that)\n",
    "        elif check_for_non_latin_characters(journal_name) == 1:\n",
    "            return emb_model.encode(journal_name)\n",
    "\n",
    "        elif journal_name == '':\n",
    "            return np.zeros(384, dtype=np.float32)\n",
    "\n",
    "        else:\n",
    "            return np.zeros(384, dtype=np.float32)\n",
    "    else:\n",
    "        return np.zeros(384, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81dfd55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(dictionary, file_path):\n",
    "    # Save the dictionary as a pickle file\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "        \n",
    "def open_pickle(pickle_path):\n",
    "    # Open the pickle file\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        pickle_dict = pickle.load(f)\n",
    "    return pickle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7b7810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(seq, **kwargs):\n",
    "    tok_data = tokenizer(seq, max_length=512, padding='max_length', truncation=True, **kwargs)\n",
    "    return [tok_data['input_ids'], tok_data['attention_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec9d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_level_0_to_1(level_0, level_1):\n",
    "    return list(set(level_0 + level_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0abfbd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_citations_for_model(list_of_links, num_to_take):\n",
    "    if len(list_of_links) <= num_to_take:\n",
    "        return list_of_links\n",
    "    else:\n",
    "        return random.sample(list_of_links, num_to_take)\n",
    "\n",
    "def get_final_citations_feature(citations, num_to_keep):\n",
    "    if citations:\n",
    "        new_citations = get_final_citations_for_model(citations, num_to_keep)\n",
    "        mapped_cites = [gold_to_label_mapping.get(x) for x in new_citations \n",
    "                        if gold_to_label_mapping.get(x)]\n",
    "        temp_feature = [citation_feature_vocab[x] for x in mapped_cites]\n",
    "    \n",
    "        if len(temp_feature) < num_to_keep:\n",
    "            return temp_feature + [0]*(num_to_keep - len(temp_feature))\n",
    "        else:\n",
    "            return temp_feature\n",
    "    else:\n",
    "        return [1] + [0]*(num_to_keep - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb3f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_title_and_abstract(title, abstract):\n",
    "    if isinstance(title, str):\n",
    "        if isinstance(abstract, str):\n",
    "            if len(abstract) >=30:\n",
    "                return f\"<TITLE> {title}\\n<ABSTRACT> {abstract[:2500]}\"\n",
    "            else:\n",
    "                return f\"<TITLE> {title}\"\n",
    "        else:\n",
    "            return f\"<TITLE> {title}\"\n",
    "    else:\n",
    "        if isinstance(abstract, str):\n",
    "            if len(abstract) >=30:\n",
    "                return f\"<TITLE> NONE\\n<ABSTRACT> {abstract[:2500]}\"\n",
    "            else:\n",
    "                return \"\"\n",
    "        else:\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "039f585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(old_title):\n",
    "    keep_title = check_for_non_latin_characters(old_title)\n",
    "    if keep_title == 1:\n",
    "        new_title = remove_non_latin_characters(old_title)\n",
    "        if '<' in new_title:\n",
    "            new_title = new_title.replace(\"<i>\", \"\").replace(\"</i>\",\"\")\\\n",
    "                                 .replace(\"<sub>\", \"\").replace(\"</sub>\",\"\") \\\n",
    "                                 .replace(\"<sup>\", \"\").replace(\"</sup>\",\"\") \\\n",
    "                                 .replace(\"<em>\", \"\").replace(\"</em>\",\"\") \\\n",
    "                                 .replace(\"<b>\", \"\").replace(\"</b>\",\"\") \\\n",
    "                                 .replace(\"<I>\", \"\").replace(\"</I>\", \"\") \\\n",
    "                                 .replace(\"<SUB>\", \"\").replace(\"</SUB>\", \"\") \\\n",
    "                                 .replace(\"<scp>\", \"\").replace(\"</scp>\", \"\") \\\n",
    "                                 .replace(\"<font>\", \"\").replace(\"</font>\", \"\") \\\n",
    "                                 .replace(\"<inf>\",\"\").replace(\"</inf>\", \"\") \\\n",
    "                                 .replace(\"<i /> \", \"\") \\\n",
    "                                 .replace(\"<p>\", \"\").replace(\"</p>\",\"\") \\\n",
    "                                 .replace(\"<![CDATA[<B>\", \"\").replace(\"</B>]]>\", \"\") \\\n",
    "                                 .replace(\"<italic>\", \"\").replace(\"</italic>\",\"\")\\\n",
    "                                 .replace(\"<title>\", \"\").replace(\"</title>\", \"\") \\\n",
    "                                 .replace(\"<br>\", \"\").replace(\"</br>\",\"\").replace(\"<br/>\",\"\") \\\n",
    "                                 .replace(\"<B>\", \"\").replace(\"</B>\", \"\") \\\n",
    "                                 .replace(\"<em>\", \"\").replace(\"</em>\", \"\") \\\n",
    "                                 .replace(\"<BR>\", \"\").replace(\"</BR>\", \"\") \\\n",
    "                                 .replace(\"<title>\", \"\").replace(\"</title>\", \"\") \\\n",
    "                                 .replace(\"<strong>\", \"\").replace(\"</strong>\", \"\") \\\n",
    "                                 .replace(\"<formula>\", \"\").replace(\"</formula>\", \"\") \\\n",
    "                                 .replace(\"<roman>\", \"\").replace(\"</roman>\", \"\") \\\n",
    "                                 .replace(\"<SUP>\", \"\").replace(\"</SUP>\", \"\") \\\n",
    "                                 .replace(\"<SSUP>\", \"\").replace(\"</SSUP>\", \"\") \\\n",
    "                                 .replace(\"<sc>\", \"\").replace(\"</sc>\", \"\") \\\n",
    "                                 .replace(\"<subtitle>\", \"\").replace(\"</subtitle>\", \"\") \\\n",
    "                                 .replace(\"<emph/>\", \"\").replace(\"<emph>\", \"\").replace(\"</emph>\", \"\") \\\n",
    "                                 .replace(\"\"\"<p class=\"Body\">\"\"\", \"\") \\\n",
    "                                 .replace(\"<TITLE>\", \"\").replace(\"</TITLE>\", \"\") \\\n",
    "                                 .replace(\"<sub />\", \"\").replace(\"<sub/>\", \"\") \\\n",
    "                                 .replace(\"<mi>\", \"\").replace(\"</mi>\", \"\") \\\n",
    "                                 .replace(\"<bold>\", \"\").replace(\"</bold>\", \"\") \\\n",
    "                                 .replace(\"<mtext>\", \"\").replace(\"</mtext>\", \"\") \\\n",
    "                                 .replace(\"<msub>\", \"\").replace(\"</msub>\", \"\") \\\n",
    "                                 .replace(\"<mrow>\", \"\").replace(\"</mrow>\", \"\") \\\n",
    "                                 .replace(\"</mfenced>\", \"\").replace(\"</math>\", \"\")\n",
    "\n",
    "            if '<mml' in new_title:\n",
    "                all_parts = [x for y in [i.split(\"mml:math>\") for i in new_title.split(\"<mml:math\")] for x in y if x]\n",
    "                final_parts = []\n",
    "                for part in all_parts:\n",
    "                    if re.search(r\"\\>[$%#!^*\\w.,/()+-]*\\<\", part):\n",
    "                        pull_out = re.findall(r\"\\>[$%#!^*\\w.,/()+-]*\\<\", part)\n",
    "                        final_pieces = []\n",
    "                        for piece in pull_out:\n",
    "                            final_pieces.append(piece.replace(\">\", \"\").replace(\"<\", \"\"))\n",
    "                        \n",
    "                        final_parts.append(\" \"+ \"\".join(final_pieces) + \" \")\n",
    "                    else:\n",
    "                        final_parts.append(part)\n",
    "                \n",
    "                new_title = \"\".join(final_parts).strip()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if '<xref' in new_title:\n",
    "                new_title = re.sub(r\"\\<xref[^/]*\\/xref\\>\", \"\", new_title)\n",
    "\n",
    "            if '<inline-formula' in new_title:\n",
    "                new_title = re.sub(r\"\\<inline-formula[^/]*\\/inline-formula\\>\", \"\", new_title)\n",
    "\n",
    "            if '<title' in new_title:\n",
    "                new_title = re.sub(r\"\\<title[^/]*\\/title\\>\", \"\", new_title)\n",
    "\n",
    "            if '<p class=' in new_title:\n",
    "                new_title = re.sub(r\"\\<p class=[^>]*\\>\", \"\", new_title)\n",
    "            \n",
    "            if '<span class=' in new_title:\n",
    "                new_title = re.sub(r\"\\<span class=[^>]*\\>\", \"\", new_title)\n",
    "\n",
    "            if 'mfenced open' in new_title:\n",
    "                new_title = re.sub(r\"\\<mfenced open=[^>]*\\>\", \"\", new_title)\n",
    "            \n",
    "            if 'math xmlns' in new_title:\n",
    "                new_title = re.sub(r\"\\<math xmlns=[^>]*\\>\", \"\", new_title)\n",
    "\n",
    "        if '<' in new_title:\n",
    "            new_title = new_title.replace(\">i<\", \"\").replace(\">/i<\", \"\") \\\n",
    "                                 .replace(\">b<\", \"\").replace(\">/b<\", \"\") \\\n",
    "                                 .replace(\"<inline-formula>\", \"\").replace(\"</inline-formula>\",\"\")\n",
    "\n",
    "        return new_title\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c12e5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_abstract(raw_abstract, inverted=False):\n",
    "    if inverted:\n",
    "        if isinstance(raw_abstract, dict) | isinstance(raw_abstract, str):\n",
    "            if isinstance(raw_abstract, dict):\n",
    "                invert_abstract = raw_abstract\n",
    "            else:\n",
    "                invert_abstract = json.loads(raw_abstract)\n",
    "            \n",
    "            if invert_abstract.get('IndexLength'):\n",
    "                ab_len = invert_abstract['IndexLength']\n",
    "\n",
    "                if ab_len > 15:\n",
    "                    abstract = [\" \"]*ab_len\n",
    "                    for key, value in invert_abstract['InvertedIndex'].items():\n",
    "                        for i in value:\n",
    "                            abstract[i] = key\n",
    "                    final_abstract = \" \".join(abstract)[:2500]\n",
    "                else:\n",
    "                    final_abstract = None\n",
    "            else:\n",
    "                if len(invert_abstract) > 15:\n",
    "                    abstract = [\" \"]*1200\n",
    "                    for key, value in invert_abstract.items():\n",
    "                        for i in value:\n",
    "                            try:\n",
    "                                abstract[i] = key\n",
    "                            except:\n",
    "                                pass\n",
    "                    final_abstract = \" \".join(abstract)[:2500]\n",
    "                else:\n",
    "                    final_abstract = None\n",
    "                \n",
    "        else:\n",
    "            final_abstract = None\n",
    "    else:\n",
    "        ab_len = len(raw_abstract)\n",
    "        if ab_len > 30:\n",
    "            final_abstract = raw_abstract[:2500]\n",
    "        else:\n",
    "            final_abstract = None\n",
    "            \n",
    "    return final_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a73ebb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_feature(features):\n",
    "\n",
    "    # Convert to a tensorflow feature\n",
    "    input_feature = [tf.expand_dims(tf.convert_to_tensor(x), axis=0) for x in [np.array(features[0], dtype=np.int32), \n",
    "                                                                             np.array(features[1], dtype=np.int32), \n",
    "                                                                             features[2]]]\n",
    "\n",
    "    return input_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b79c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_citations_from_all_citations(all_citations, gold_dict, non_gold_dict):\n",
    "    if isinstance(all_citations, list):\n",
    "        if len(all_citations) > 200:\n",
    "            all_citations = random.sample(all_citations, 200)\n",
    "        \n",
    "        level_0_gold_temp = [[x, gold_dict.get(x)] for x in all_citations if gold_dict.get(x)]\n",
    "\n",
    "        level_1_gold_temp = [non_gold_dict.get(x) for x in all_citations if non_gold_dict.get(x)]\n",
    "\n",
    "        level_0_gold = [x[0] for x in level_0_gold_temp]\n",
    "        level_1_gold = [y for z in [x[1] for x in level_0_gold_temp] for y in z] + \\\n",
    "                        [x for y in level_1_gold_temp for x in y]\n",
    "\n",
    "        return level_0_gold, level_1_gold\n",
    "    else:\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9342338c-c070-4eaa-b1e9-340810d610da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_model_output(input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Returning XLA optimized output from language model\n",
    "    \n",
    "    Input:\n",
    "    input_ids: tokenized title/abstract\n",
    "    attention_mask: tokenized title/abstract attention mask\n",
    "    \n",
    "    Output:\n",
    "    last layer output from language model\n",
    "    \"\"\"\n",
    "    return xla_predict_lang_model(input_ids=input_ids, attention_mask=attention_mask).hidden_states[-1]\n",
    "    \n",
    "\n",
    "def create_model(num_classes, emb_table_size, model_chkpt, topk=5):\n",
    "    \"\"\"\n",
    "    Function to create full model.\n",
    "    \n",
    "    Input:\n",
    "    num_classes: number of classes\n",
    "    emb_table_size: size of embedding table\n",
    "    model_chkpt: path to model checkpoint\n",
    "    topk: number of predictions to return\n",
    "    \n",
    "    Output:\n",
    "    model: full model\n",
    "    \"\"\"\n",
    "    # Inputs\n",
    "    citation_0 = tf.keras.layers.Input((16,), dtype=tf.int64, name='citation_0')\n",
    "    citation_1 = tf.keras.layers.Input((128,), dtype=tf.int64, name='citation_1')\n",
    "    journal = tf.keras.layers.Input((384,), dtype=tf.float32, name='journal_emb')\n",
    "    language_model_output = tf.keras.layers.Input((512, 768,), dtype=tf.float32, name='lang_model_output')\n",
    "    \n",
    "    # Create a multi-class classification model using functional API\n",
    "    pooled_language_model_output = tf.keras.layers.GlobalAveragePooling1D()(language_model_output)\n",
    "    citation_emb_layer = tf.keras.layers.Embedding(input_dim=emb_table_size, output_dim=256, mask_zero=True, \n",
    "                                                   trainable=True, name='citation_emb_layer')\n",
    "\n",
    "    citation_0_emb = citation_emb_layer(citation_0)\n",
    "    citation_1_emb = citation_emb_layer(citation_1)\n",
    "\n",
    "    pooled_citation_0 = tf.keras.layers.GlobalAveragePooling1D()(citation_0_emb)\n",
    "    pooled_citation_1 = tf.keras.layers.GlobalAveragePooling1D()(citation_1_emb)\n",
    "\n",
    "    concat_data = tf.keras.layers.Concatenate(name='concat_data', axis=-1)([pooled_language_model_output, pooled_citation_0, \n",
    "                                                                            pooled_citation_1, journal])\n",
    "\n",
    "    # Dense layer 1\n",
    "    dense_output = tf.keras.layers.Dense(2048, activation='relu', kernel_regularizer='L2', name=\"dense_1\")(concat_data)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_1\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_1\")(dense_output)\n",
    "    \n",
    "    # Dense layer 2\n",
    "    dense_output = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer='L2', name=\"dense_2\")(dense_output)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_2\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_2\")(dense_output)\n",
    "\n",
    "    # Dense layer 3\n",
    "    dense_output_l3 = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer='L2', name=\"dense_3\")(dense_output)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_3\")(dense_output_l3)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_3\")(dense_output)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(num_classes, activation='sigmoid', name='output_layer')(dense_output)\n",
    "    topk_outputs = tf.math.top_k(output_layer, k=topk)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[citation_0, citation_1, journal, language_model_output], \n",
    "                           outputs=topk_outputs)\n",
    "\n",
    "    model.load_weights(model_chkpt)\n",
    "    model.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_final_ids_and_scores_bad(topic_ids, score, labels, title, abstract, threshold=0.04):\n",
    "    \"\"\"\n",
    "    Function to apply some rules to get the final prediction (some clusters performed worse than others).\n",
    "    \n",
    "    Input:\n",
    "    topic_ids: all ids for raw prediction output\n",
    "    score: all scores for raw prediction output\n",
    "    labels: all labels for raw prediction output\n",
    "    title: title of the work\n",
    "    abstract: abstract of the work\n",
    "    \n",
    "    Output:\n",
    "    final_ids: post-processed final ids\n",
    "    final_scores: post-processed final scores\n",
    "    final_labels: post-processed final labels\n",
    "    \"\"\"\n",
    "    final_ids = [-1]\n",
    "    final_scores = [0.0]\n",
    "    final_labels = [None]\n",
    "    if any(topic_id in topic_ids for topic_id in [13241]):\n",
    "        return final_ids, final_scores, final_labels\n",
    "    elif any(topic_id in topic_ids for topic_id in [12705,13003]):\n",
    "        if title != '':\n",
    "            if check_for_non_latin_characters(title) == 1:\n",
    "                if len(title.split(\" \")) > 9:\n",
    "                    if not isinstance(abstract, str):\n",
    "                        final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                        final_scores = [y for y in score if y > threshold]\n",
    "                        final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "                        if final_ids:\n",
    "                            return final_ids, final_scores, final_labels\n",
    "                        else:\n",
    "                            return [-1], [0.0], [None]\n",
    "                    elif isinstance(abstract, str):\n",
    "                        if check_for_non_latin_characters(abstract) == 1:\n",
    "                            final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                            final_scores = [y for y in score if y > 0.05]\n",
    "                            final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "                            if final_ids:\n",
    "                                return final_ids, final_scores, final_labels\n",
    "                            else:\n",
    "                                return [-1], [0.0], [None]\n",
    "                        else:\n",
    "                            return final_ids, final_scores, final_labels\n",
    "                    else:\n",
    "                        return final_ids, final_scores, final_labels\n",
    "                else:\n",
    "                    return final_ids, final_scores, final_labels\n",
    "            else:\n",
    "                return final_ids, final_scores, final_labels\n",
    "        else:\n",
    "            return final_ids, final_scores, final_labels\n",
    "    else:\n",
    "        if any(topic_id in topic_ids for topic_id in [12718,14377,13686,13723]):\n",
    "            final_ids = [x for x,y in zip(topic_ids, score) if (x not in [12718,14377,13686,13723]) & (y > 0.80)]\n",
    "            final_scores = [y for x,y in zip(topic_ids, score) if (x not in [12718,14377,13686,13723]) & (y > 0.80)]\n",
    "            final_labels = [y for x,y,z in zip(topic_ids, labels, score) if (x not in [12718,14377,13686,13723]) & (z > 0.80)]\n",
    "            if final_ids:\n",
    "                return final_ids, final_scores, final_labels\n",
    "            else:\n",
    "                return [-1], [0.0], [None]\n",
    "        elif any(topic_id in topic_ids for topic_id in [13064, 13537]):\n",
    "            if title == 'Frontmatter':\n",
    "                return [-1], [0.0], [None]\n",
    "            else:\n",
    "                final_ids = [x for x,y in zip(topic_ids, score) if (((x in [13064, 13537]) & (y > 0.95)) | \n",
    "                                                                ((x not in [13064, 13537]) & (y > threshold)))]\n",
    "                final_scores = [y for x,y in zip(topic_ids, score) if (((x in [13064, 13537]) & (y > 0.95)) | \n",
    "                                                                    ((x not in [13064, 13537]) & (y > threshold)))]\n",
    "                final_labels = [z for x,y,z in zip(topic_ids, score, labels) if (((x in [13064, 13537]) & (y > 0.95)) | \n",
    "                                                                    ((x not in [13064, 13537]) & (y > threshold)))]\n",
    "                if final_ids:\n",
    "                    return final_ids, final_scores, final_labels\n",
    "                else:\n",
    "                    return [-1], [0.0], [None]\n",
    "        elif any(topic_id in topic_ids for topic_id in [11893, 13459]):\n",
    "            test_scores = [y for x,y in zip(topic_ids, score) if (x in [11893, 13459])]\n",
    "            if topic_ids[0] in [11893, 13459]:\n",
    "                first_pred = 1\n",
    "            else:\n",
    "                first_pred = 0\n",
    "            \n",
    "            if [x for x in test_scores if x > 0.95] & (first_pred == 1):\n",
    "                final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                final_scores = [y for y in score if y > 0.05]\n",
    "                final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "\n",
    "                if final_ids:\n",
    "                    return final_ids, final_scores, final_labels\n",
    "                else:\n",
    "                    return [-1], [0.0], [None]\n",
    "            elif first_pred == 0:\n",
    "                final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                final_scores = [y for y in score if y > threshold]\n",
    "                final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "\n",
    "                if final_ids:\n",
    "                    return final_ids, final_scores, final_labels\n",
    "                else:\n",
    "                    return [-1], [0.0], [None]\n",
    "            else:\n",
    "                return [-1], [0.0], [None]\n",
    "        else:\n",
    "            if isinstance(abstract, str) & (title != ''):\n",
    "                if (check_for_non_latin_characters(title) == 1) & (check_for_non_latin_characters(abstract) == 1):\n",
    "                    final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                    final_scores = [y for y in score if y > threshold]\n",
    "                    final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "    \n",
    "                    if final_ids:\n",
    "                        return final_ids, final_scores, final_labels\n",
    "                    else:\n",
    "                        return [-1], [0.0], [None]\n",
    "                else:\n",
    "                    return [-1], [0.0], [None]\n",
    "            elif title != '':\n",
    "                if (check_for_non_latin_characters(title) == 1):\n",
    "                    final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                    final_scores = [y for y in score if y > threshold]\n",
    "                    final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "    \n",
    "                    if final_ids:\n",
    "                        return final_ids, final_scores, final_labels\n",
    "                    else:\n",
    "                        return [-1], [0.0], [None]\n",
    "                else:\n",
    "                    return [-1], [0.0], [None]\n",
    "            elif isinstance(abstract, str):\n",
    "                if (check_for_non_latin_characters(abstract) == 1):\n",
    "                    final_ids = [x for x,y in zip(topic_ids, score) if y > threshold]\n",
    "                    final_scores = [y for y in score if y > threshold]\n",
    "                    final_labels = [x for x,y in zip(labels, score) if y > threshold]\n",
    "    \n",
    "                    if final_ids:\n",
    "                        return final_ids, final_scores, final_labels\n",
    "                    else:\n",
    "                        return [-1], [0.0], [None]\n",
    "                else:\n",
    "                    return [-1], [0.0], [None]\n",
    "            else:\n",
    "                return [-1], [0.0], [None]\n",
    "\n",
    "def process_data_as_df(new_df):\n",
    "    \"\"\"\n",
    "    Function to process data as a dataframe (in batch).\n",
    "    \n",
    "    Input:\n",
    "    new_df: dataframe of data\n",
    "    \n",
    "    Output:\n",
    "    input_df: dataframe of data with predictions\n",
    "    \"\"\"\n",
    "    input_df = new_df.copy()\n",
    "    # Get citations into integer format\n",
    "    input_df['referenced_works'] = input_df['referenced_works'].apply(lambda x: [int(i.split(\"https://openalex.org/W\")[1]) for \n",
    "                                                                             i in x])\n",
    "\n",
    "     # Process title and abstract and tokenize\n",
    "    input_df['title'] = input_df['title'].apply(lambda x: clean_title(x))\n",
    "    input_df['abstract_inverted_index'] = input_df.apply(lambda x: clean_abstract(x.abstract_inverted_index, x.inverted), axis=1)\n",
    "    title_abstract = input_df.apply(lambda x: merge_title_and_abstract(x.title, x.abstract_inverted_index), axis=1).tolist()\n",
    "\n",
    "    # pipeline_output = test_pipeline(title_abstract)\n",
    "    tok_inputs_pt = tokenize(title_abstract, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        last_output = pt_model(*tok_inputs_pt).hidden_states[-1]\n",
    "    lang_model_output = last_output.numpy()\n",
    "    \n",
    "    # Take citations and return only gold citations (and then convert to label ids)\n",
    "    input_df['referenced_works'] = input_df['referenced_works'].apply(lambda x: get_gold_citations_from_all_citations(x, gold_dict, \n",
    "                                                                                                                      non_gold_dict))\n",
    "    input_df['citation_0'] = input_df['referenced_works'].apply(lambda x: get_final_citations_feature(x[0], 16))\n",
    "    input_df['citation_1'] = input_df['referenced_works'].apply(lambda x: get_final_citations_feature(x[1], 128))    \n",
    "    \n",
    "    # Take in journal name and output journal embedding\n",
    "    input_df['journal_emb'] = input_df['journal_display_name'].apply(get_journal_emb)\n",
    "\n",
    "    # Check completeness of input data\n",
    "    input_df['score_data'] = input_df\\\n",
    "        .apply(lambda x: 0 if ((x.title == \"\") & \n",
    "                               (not x.abstract_inverted_index) & \n",
    "                               (x.citation_0[0]==1) & \n",
    "                               (x.citation_1[0]==1)) else 1, axis=1)\n",
    "\n",
    "    data_to_score = input_df[input_df['score_data']==1].copy()\n",
    "    data_to_not_score = input_df[input_df['score_data']==0][['UID']].copy()\n",
    "\n",
    "    if data_to_score.shape[0] > 0:\n",
    "        # Transform into output for model\n",
    "        data_to_score['input_feature'] = data_to_score.apply(lambda x: create_input_feature([x.citation_0, x.citation_1, \n",
    "                                                                                             x.journal_emb]), axis=1)\n",
    "        \n",
    "        all_rows = [tf.convert_to_tensor([x[0][0] for x in data_to_score['input_feature'].tolist()]), \n",
    "                    tf.convert_to_tensor([x[1][0] for x in data_to_score['input_feature'].tolist()]), \n",
    "                    tf.convert_to_tensor([x[2][0] for x in data_to_score['input_feature'].tolist()]), \n",
    "                    tf.convert_to_tensor(lang_model_output)]\n",
    "        \n",
    "        preds = xla_predict(all_rows)\n",
    "        \n",
    "        data_to_score['preds'] = preds.indices.numpy().tolist()\n",
    "        data_to_score['scores'] = preds.values.numpy().tolist()\n",
    "    else:\n",
    "        data_to_score['preds'] = [[-1]]*data_to_not_score.shape[0]\n",
    "        data_to_score['scores'] = [[0.0000]]*data_to_not_score.shape[0]\n",
    "    \n",
    "    data_to_not_score['preds'] = [[-1]]*data_to_not_score.shape[0]\n",
    "    data_to_not_score['scores'] = [[0.0000]]*data_to_not_score.shape[0]\n",
    "    \n",
    "    return input_df[['UID','title','abstract_inverted_index']].merge(pd.concat([data_to_score[['UID','preds','scores']], \n",
    "                                              data_to_not_score[['UID','preds','scores']]], axis=0), \n",
    "                                   how='left', on='UID')\n",
    "\n",
    "def last_pred_check(old_preds, old_scores, old_labels):\n",
    "    \"\"\"\n",
    "    Function to apply some rules to get the final prediction based on scores\n",
    "    \n",
    "    Input:\n",
    "    old_preds: all ids for prediction output\n",
    "    old_scores: all scores for prediction output\n",
    "    old_labels: all labels for prediction output\n",
    "    \n",
    "    Output:\n",
    "    final_ids: post-processed final ids\n",
    "    final_scores: post-processed final scores\n",
    "    final_labels: post-processed final labels\n",
    "    \"\"\"\n",
    "    pred_scores = [[x,y,z] for x,y,z in zip(old_preds, old_scores, old_labels)]\n",
    "\n",
    "    # if any of scores are over 0.9\n",
    "    if [x[1] for x in pred_scores if x[1] > 0.9]:\n",
    "        final_pred_scores = [[x[0], x[1], x[2]] for x in pred_scores if x[1] > 0.9]\n",
    "    elif len(pred_scores) == 1:\n",
    "        final_pred_scores = pred_scores.copy()\n",
    "    elif len(pred_scores) == 2:\n",
    "        scores = [x[1] for x in pred_scores]\n",
    "        if scores[1] < (scores[0]/2):\n",
    "            final_pred_scores = pred_scores[:1].copy()\n",
    "        else:\n",
    "            final_pred_scores = pred_scores.copy()\n",
    "    else:\n",
    "        preds = [x[0] for x in pred_scores]\n",
    "        scores = [x[1] for x in pred_scores]\n",
    "        labels = [x[2] for x in pred_scores]\n",
    "\n",
    "        score_sum = scores[0]\n",
    "        final_pred_scores = pred_scores[:1].copy()\n",
    "        for i, (pred, score, label) in enumerate(zip(preds[1:], scores[1:], labels[1:])):\n",
    "            if score < (score_sum/(i+1)*0.85):\n",
    "                break\n",
    "            else:\n",
    "                final_pred_scores.append([pred, score, label])\n",
    "                score_sum += score\n",
    "\n",
    "    final_preds = [x[0] for x in final_pred_scores]\n",
    "    final_scores = [x[1] for x in final_pred_scores]\n",
    "    final_labels = [x[2] for x in final_pred_scores]\n",
    "    return final_preds, final_scores, final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f173550a-5afd-442e-8c66-0a4f95b4d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98575250-69df-41ee-bcca-63852f838977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing a custom pipeline for speed\n",
    "class CustomHiddenOutputPipeline(TextClassificationPipeline):\n",
    "    def __init__(self, model_path, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            model=TFAutoModelForSequenceClassification.from_pretrained(model_path, output_hidden_states=True_path, truncate=True),\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.model.trainable = False\n",
    "\n",
    "    def preprocess(self, inputs, **tokenizer_kwargs):\n",
    "        return_tensors = self.framework\n",
    "        if isinstance(inputs, dict):\n",
    "            return self.tokenizer(**inputs, return_tensors=return_tensors, **tokenizer_kwargs)\n",
    "        elif isinstance(inputs, list) and len(inputs) == 1 and isinstance(inputs[0], list) and len(inputs[0]) == 2:\n",
    "            # It used to be valid to use a list of list of list for text pairs, keeping this path for BC\n",
    "            return self.tokenizer(\n",
    "                text=inputs[0][0], text_pair=inputs[0][1], return_tensors=return_tensors, **tokenizer_kwargs\n",
    "            )\n",
    "        elif isinstance(inputs, list):\n",
    "            # This is likely an invalid usage of the pipeline attempting to pass text pairs.\n",
    "            raise ValueError(\n",
    "                \"The pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a\"\n",
    "                ' dictionary `{\"text\": \"My text\", \"text_pair\": \"My pair\"}` in order to send a text pair.'\n",
    "            )\n",
    "        return self.tokenizer(inputs, return_tensors=return_tensors, max_length=512, truncation=True, padding='max_length')),\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        # Forward\n",
    "        outputs = self.model(**model_inputs)\n",
    "        hidden_state = outputs[1]\n",
    "\n",
    "        return {\n",
    "            \"hidden_state\": hidden_state\n",
    "        }\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        outputs = model_outputs[\"hidden_state\"][-1].numpy()\n",
    "        return {\"hidden_state\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "10d43004-305e-4c25-85fd-a9e68b7e3a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "test_pipeline = CustomHiddenOutputPipeline(model_path='OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8dea30e5-8405-409b-893e-7435c66f3922",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### The following are empty inputs for each feature (title, abstract, citation_0, citation_1, journal_emb\n",
    "# [101, 102] + [0]*510\n",
    "# [1, 1] + [0]*510\n",
    "# [1]+[0]*15,\n",
    "# [1]+[0]*127, \n",
    "# np.zeros(384, dtype=np.float32)\n",
    "\n",
    "artifacts_folder = './full_model_iter6' # Change this to location of model artifacts\n",
    "\n",
    "target_vocab = open_pickle(f'{artifacts_folder}/model_artifacts/target_vocab.pkl')\n",
    "inv_target_vocab = open_pickle(f'{artifacts_folder}/model_artifacts/inv_target_vocab.pkl')\n",
    "citation_feature_vocab = open_pickle(f'{artifacts_folder}/model_artifacts/citation_feature_vocab.pkl')\n",
    "gold_to_label_mapping = open_pickle(f'{artifacts_folder}/model_artifacts/gold_to_id_mapping_dict.pkl')\n",
    "emb_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "language_model_name = \\\n",
    "    \"OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(language_model_name , truncate=True, pad=True)\n",
    "gold_dict = open_pickle(f'{artifacts_folder}/model_artifacts/gold_citations_dict.pkl')\n",
    "non_gold_dict = open_pickle(f'{artifacts_folder}/model_artifacts/non_gold_citations_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7ddd0e8-21fc-4667-b2df-c42a32337161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "test_pipeline = CustomHiddenOutputPipeline(language_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdbe4aa0-867d-49a6-982c-661bdb2071b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Loading the models\n",
    "language_model_name = \\\n",
    "    \"OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract\"\n",
    "\n",
    "pred_model = create_model(len(target_vocab), \n",
    "                          len(citation_feature_vocab)+2,\n",
    "                          \"citation_part_only.keras\", topk=5)\n",
    "xla_predict = tf.function(pred_model, jit_compile=True)\n",
    "\n",
    "# test_pipeline = CustomHiddenOutputPipeline(language_model_name)\n",
    "\n",
    "language_model = TFAutoModelForSequenceClassification.from_pretrained(language_model_name, output_hidden_states=True)\n",
    "language_model.trainable = False\n",
    "xla_predict_lang_model = tf.function(language_model, jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "de49a03d-0511-4f1d-ade0-63044e4b3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.bettertransformer import BetterTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b93ba242-e676-4fe3-9ad5-dfc2339746d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"<TITLE> The Shape of the Olfactory Bulb Predicts Olfactory Function\n",
    "<ABSTRACT> The olfactory bulb (OB) plays a key role in the processing of olfactory information. A large body of research has shown that OB volumes correlate with olfactory function, which provides diagnostic and prognostic information in olfactory dysfunction. Still, the potential value of the OB shape remains unclear. Based on our clinical experience we hypothesized that the shape of the OB predicts olfactory function, and that it is linked to olfactory loss, age, and gender. The aim of this study was to produce a classification of OB shape in the human brain, scalable to clinical and research applications. Results from patients with the five most frequent causes of olfactory dysfunction (n = 192) as well as age/gender-matched healthy controls (n = 77) were included. Olfactory function was examined in great detail using the extended \"Sniffin' Sticks\" test. A high-resolution structural T2-weighted MRI scan was obtained for all. The planimetric contours (surface in mm2) of OB were delineated manually, and then all surfaces were added and multiplied to obtain the OB volume in mm3. OB shapes were outlined manually and characterized on a selected slice through the posterior coronal plane tangential to the eyeballs. We looked at OB shapes in terms of convexity and defined two patterns/seven categories based on OB contours: convex (olive, circle, and plano-convex) and non-convex (banana, irregular, plane, and scattered). Categorization of OB shapes is possible with a substantial inter-rater agreement (Cohen's Kappa = 0.73). Our results suggested that non-convex OB patterns were significantly more often observed in patients than in controls. OB shapes were correlated with olfactory function in the whole group, independent of age, gender, and OB volume. OB shapes seemed to change with age in healthy subjects. Importantly, the results indicated that OB shapes were associated with certain causes of olfactory disorders, i.e., an irregular OB shape was significantly more often observed in post-traumatic olfactory loss. Our study provides evidence that the shape of the OB can be used as a biomarker for olfactory dysfunction.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69e60bc2-c1bf-42aa-ae49-41cef0e5495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_inputs_pt = tokenize(test_text[:50], return_tensors='pt')\n",
    "tok_inputs_tf = tokenize(test_text[:50], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "aedfda57-33f2-45e4-82fc-37d7194602ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1aeb1e74a04eecb8779c2023193ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/617k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc9bbafc8024fb79ae09c1119efd982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/725M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pt_model = AutoModelForSequenceClassification.from_pretrained(language_model_name, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97457f3e-d99a-4544-93a1-6eeee8bebdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cac5398a-cf8e-4df8-bcbb-55c85b26c57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openalex.org/W4205779344\n",
      "CPU times: user 33.9 ms, sys: 667 µs, total: 34.6 ms\n",
      "Wall time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "open_req = \"https://api.openalex.org/works/W4205779344\"\n",
    "resp = requests.get(open_req).json()\n",
    "print(resp['id'])\n",
    "\n",
    "if resp['primary_location']['source']:\n",
    "    journal_display_name = resp['primary_location']['source']['display_name']\n",
    "else:\n",
    "    journal_display_name = \"\"\n",
    "\n",
    "\n",
    "input_json = [{'title': resp['title'], \n",
    "               'abstract_inverted_index': resp['abstract_inverted_index'], \n",
    "               'journal_display_name': journal_display_name, \n",
    "               'referenced_works': resp['referenced_works'],\n",
    "               'inverted': True}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c9c3772-58cc-4644-b16b-c02241b72c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.DataFrame.from_dict(input_json).reset_index().rename(columns={'index': 'UID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d94c7db7-9763-4df5-afc2-247fe0d3e89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f6b08cc-fa64-41ab-8b19-ad2313483495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TITLE> The Shape of the Olfactory Bulb Predicts Olfactory Function\n",
      "<ABSTRACT> The olfactory bulb (OB) plays a key role in the processing of olfactory information. A large body of research has shown that OB volumes correlate with olfactory function, which provides diagnostic and prognostic information in olfactory dysfunction. Still, the potential value of the OB shape remains unclear. Based on our clinical experience we hypothesized that the shape of the OB predicts olfactory function, and that it is linked to olfactory loss, age, and gender. The aim of this study was to produce a classification of OB shape in the human brain, scalable to clinical and research applications. Results from patients with the five most frequent causes of olfactory dysfunction (n = 192) as well as age/gender-matched healthy controls (n = 77) were included. Olfactory function was examined in great detail using the extended \"Sniffin' Sticks\" test. A high-resolution structural T2-weighted MRI scan was obtained for all. The planimetric contours (surface in mm2) of OB were delineated manually, and then all surfaces were added and multiplied to obtain the OB volume in mm3. OB shapes were outlined manually and characterized on a selected slice through the posterior coronal plane tangential to the eyeballs. We looked at OB shapes in terms of convexity and defined two patterns/seven categories based on OB contours: convex (olive, circle, and plano-convex) and non-convex (banana, irregular, plane, and scattered). Categorization of OB shapes is possible with a substantial inter-rater agreement (Cohen's Kappa = 0.73). Our results suggested that non-convex OB patterns were significantly more often observed in patients than in controls. OB shapes were correlated with olfactory function in the whole group, independent of age, gender, and OB volume. OB shapes seemed to change with age in healthy subjects. Importantly, the results indicated that OB shapes were associated with certain causes of olfactory disorders, i.e., an irregular OB shape was significantly more often observed in post-traumatic olfactory loss. Our study provides evidence that the shape of the OB can be used as a biomarker for olfactory dysfunction.                                                                                                                                                                                                                                                                                                                                                                          \n",
      "CPU times: user 2.35 s, sys: 23 ms, total: 2.38 s\n",
      "Wall time: 281 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_preds = process_data_as_df(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee298bb2",
   "metadata": {},
   "source": [
    "#### Loading language model only from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "338f64d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "classifier_multi = \\\n",
    "    pipeline(model=\"OpenAlex/bert-base-multilingual-cased-finetuned-openalex-topic-classification-title-abstract\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2d159e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 486 ms, sys: 0 ns, total: 486 ms\n",
      "Wall time: 220 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': '3404: Geodynamics of the Northern Andes and Caribbean Region',\n",
       "   'score': 0.4785984754562378},\n",
       "  {'label': \"965: Sedimentary Processes in Earth's Geology\",\n",
       "   'score': 0.1968356966972351},\n",
       "  {'label': '2014: Biogeography and Conservation of Neotropical Freshwater Fishes',\n",
       "   'score': 0.051199547946453094},\n",
       "  {'label': '109: Paleoredox and Paleoproductivity Proxies',\n",
       "   'score': 0.01857946440577507},\n",
       "  {'label': '3205: Geodynamic Evolution of Western Mediterranean Region',\n",
       "   'score': 0.016884787008166313}]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "classifier_multi(\"\"\"<TITLE>Supplemental Material: Estimating paleotidal constituents from Pliocene “tidal gauges”—an example from the paleo-Orinoco Delta, Trinidad\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57878472-9b7e-464d-bc71-19e96726fdfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
