{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc67f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "from math import ceil\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ceb31f-8530-4b8f-b675-eb622ac7d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b1745b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from tensorflow.keras import mixed_precision\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import evaluate\n",
    "from datasets import load_dataset, load_metric, list_metrics\n",
    "from transformers import create_optimizer\n",
    "from transformers import create_optimizer, TFAutoModelForSequenceClassification, DistilBertTokenizer\n",
    "from transformers import DataCollatorWithPadding, TFDistilBertForSequenceClassification\n",
    "from transformers import TFRobertaForSequenceClassification, RobertaTokenizer, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bebbbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.35.2\n",
    "# !pip install datasets==2.15.0\n",
    "# !pip install evaluate\n",
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8636a8",
   "metadata": {},
   "source": [
    "### Processing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae71b80c-0fd8-40b2-a57d-8582986378ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_title_and_abstract(title, abstract):\n",
    "    if isinstance(title, str):\n",
    "        if isinstance(abstract, str):\n",
    "            return f\"<TITLE> {title}\\n<ABSTRACT> {abstract}\"\n",
    "        else:\n",
    "            return f\"<TITLE> {title}\"\n",
    "    else:\n",
    "        if isinstance(abstract, str):\n",
    "            return f\"<TITLE> NONE\\n<ABSTRACT> {abstract}\"\n",
    "        else:\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f433836e-9ef6-4ce5-b74f-b60b93b2d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(df, column):\n",
    "    # Create a vocab out of the column\n",
    "    vocab = df[column].unique()\n",
    "\n",
    "    # Create a dict that maps vocab to integers\n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    inv_vocab_to_int = {i:j for j,i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, inv_vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a716e2f5-b5c6-4dfc-90d3-115904048e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(dictionary, file_path):\n",
    "    # Save the dictionary as a pickle file\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e921521b-0655-4b36-b3ee-5ab81a4a4546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle(pickle_path):\n",
    "    # Open the pickle file\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        pickle_dict = pickle.load(f)\n",
    "\n",
    "    return pickle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "298f431a-d12e-4d92-9e5c-e8cb846f8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_parquet(\"{path_to_all_training_data_from_003_spark_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5648a4f-2b6f-43b6-95d3-79d55f73151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['processed_data'] = all_data.apply(lambda x: merge_title_and_abstract(x.new_title, x.abstract), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60df0d47-388d-4597-9b3e-77ddbaaa7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['full_label'] = all_data.apply(lambda x: f\"{x.micro_cluster_id}: {x.long_label}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df0da9f-0e6b-40e2-bfb8-3a8356a276e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4521"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['full_label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeee333d-f814-41ee-937c-6c42050c687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data = all_data.sample(all_data.shape[0], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef574c33-1aa1-41f1-8ad3-99d90738e59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4521000, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea79d1a3-eef8-4654-8d28-96386482f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = shuffled_data.iloc[:4300000].copy()\n",
    "val = shuffled_data.iloc[4300000:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb985a18-d3e9-48ac-bb91-3a18e8734ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4521"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['full_label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "828600d9-71a3-4c1b-91a6-89f6bcf5f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab, inv_target_vocab = create_vocab(train, 'full_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "118d21cd-afcc-437d-81e4-575323066c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = train['full_label'].apply(lambda x: target_vocab[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bfbe3ed-531e-479f-a067-9aa83ed0c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val['label'] = val['full_label'].apply(lambda x: target_vocab[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b55b9bd-6753-4bb3-9f94-7c5744411392",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(98):\n",
    "    train[['paper_id','processed_data','label']] \\\n",
    "        .iloc[44000*i:44000*(i+1)] \\\n",
    "        .to_parquet(f\"./training_data/train/train_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91f891dc-0318-4f83-9e91-203fcc05348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    val[['paper_id','processed_data','label']] \\\n",
    "        .iloc[30000*i:30000*(i+1)] \\\n",
    "        .to_parquet(f\"./training_data/val/val_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a29762e-e2fa-4e09-8858-e742ca309d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = save_pickle(target_vocab, './training_data/target_vocab.pkl')\n",
    "_ = save_pickle(inv_target_vocab , './training_data/inv_target_vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155ced38",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7217016b-ab87-4e0d-b517-aadecd5ae995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"processed_data\"], truncation=True, padding='longest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef1f716e-abdc-4946-95d3-a01ddbcb9759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "task = \"openalex-topic-classification\"\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac8abc46-c42e-4dc9-abeb-c29195efeded",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = open_pickle('./training_data/target_vocab.pkl')\n",
    "inv_target_vocab = open_pickle('./training_data/inv_target_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb6083a-cf45-44d6-9a6a-ed1a1a5d9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the standard DistilBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0682a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the HuggingFace library to load the dataset\n",
    "all_dataset = load_dataset(\"parquet\", data_files={'train': [f'./training_data/train/train_{i}.parquet' for i in range(98)], \n",
    "                                                'val': [f'./training_data/val/val_{i}.parquet' for i in range(8)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f4c168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the train dataset\n",
    "tokenized_data = all_dataset.map(preprocess_function, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26c08734-7336-4a64-965b-e64b70f0bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = tokenized_data['train'].num_rows // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9c42490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95d1b0-2503-47ba-a3f1-7649f82f1e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Allow for use of multiple GPUs\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    # Loading the model and weights with a classification head\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "                                                                 num_labels=len(inv_target_vocab), \n",
    "                                                                 id2label=inv_target_vocab, \n",
    "                                                                 label2id=target_vocab)\n",
    "    model.bert.embeddings.trainable = False\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./model_checkpoints/{epoch:02d}-{val_loss:.2f}.keras',\n",
    "        save_weights_only=False,\n",
    "        save_best_only=False)\n",
    "\n",
    "    tf_train_set = model.prepare_tf_dataset(\n",
    "        tokenized_data[\"train\"],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "    \n",
    "    tf_validation_set = model.prepare_tf_dataset(\n",
    "        tokenized_data[\"val\"],\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "\n",
    "    optimizer, schedule = create_optimizer(init_lr=6e-5, num_warmup_steps=500, num_train_steps=total_train_steps)\n",
    "\n",
    "    metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=1)\n",
    "    callbacks = [metric_callback, model_checkpoint_callback, early_stopping]\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3171b5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  177853440 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3476649   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181330089 (691.72 MB)\n",
      "Trainable params: 89121705 (339.97 MB)\n",
      "Non-trainable params: 92208384 (351.75 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e83b14b-e730-4fe2-a7a4-be53449e5d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=num_epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2336de93-ebea-4061-aae8-9eab5000cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = save_pickle(history.history, './training_data/training_history.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bee0eeb-c07d-4774-ae72-87e9ddb2fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-multilingual-cased\"\n",
    "task = \"openalex-topic-classification-title-abstract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "237e1306-5c56-4f9f-97d2-1e7041550ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3dbd6c-2447-4c7f-a16d-f5e91bb58b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8efcdb0-ec4a-4aff-9aa5-470966679aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(f\"OpenAlex/{model_name}-finetuned-{task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f48cd-1a10-4b7e-8707-0e6841b2b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(f\"OpenAlex/{model_name}-finetuned-{task}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
