{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44920190-9120-4ae8-858f-14f52b073315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba961a2-7e64-4844-8d78-c47e5f47c752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, ArrayType, DoubleType, StructType, StructField,LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f29191d-1059-4a1a-a42b-0a11e8599c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_save_path = \"{save_path_for_openalex_tables}\"\n",
    "iteration_save_path = \"{save_path_for_most_data}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47f9abef-de4b-45bd-8d74-3d30740105f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Getting all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3ac5ec3-42ae-4d60-ab64-a195564fe3cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4521"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_labels = spark.read.parquet(f'{iteration_save_path}topic_labels_data_from_cwts_new')\n",
    "classification_labels.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc9f79d-1a5b-456f-856b-b91f4bedb928",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------------------+--------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+\n",
      "|micro_cluster_id|short_label                     |long_label                                                          |keywords                                                                                                                                                                                                                                               |summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |wikipedia_url                                                   |\n",
      "+----------------+--------------------------------+--------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+\n",
      "|2535            |Noisy Label Learning            |Learning with Noisy Labels in Machine Learning                      |Noisy Labels; Hyperparameter Optimization; Instance Selection; Robust Learning; Automated Machine Learning; Meta-Learning; Deep Neural Networks; Classification; Positive and Unlabeled Data; Loss Correction                                          |This cluster of papers focuses on the challenges and techniques for learning with noisy labels in machine learning, including methods for hyperparameter optimization, instance selection, robust learning, and automated machine learning. It also explores the use of meta-learning and deep neural networks in handling noisy label problems, particularly in the context of classification tasks and learning from positive and unlabeled data.                                                                                                                    |https://en.wikipedia.org/wiki/Learning_with_noisy_labels        |\n",
      "|3018            |Earthquake Early Warning        |Machine Learning for Earthquake Early Warning Systems               |Machine Learning; Seismic Signals; Earthquake Detection; Early Warning; Convolutional Neural Network; Seismic Event Classification; Real-Time Seismology; Deep Learning Models; Seismic Phase Picking; Citizen Science                                 |This cluster of papers focuses on the application of machine learning and deep learning techniques to improve the accuracy and timeliness of earthquake early warning systems. It covers topics such as seismic signal classification, real-time seismology, convolutional neural networks for seismic phase picking, and the integration of citizen science in earthquake monitoring.                                                                                                                                                                                 |https://en.wikipedia.org/wiki/Earthquake_early_warning_systems  |\n",
      "|1975            |Genetic Programming             |Application of Genetic Programming in Machine Learning              |Genetic Programming; Machine Learning; Classification; Evolutionary Algorithms; Feature Selection; Symbolic Regression; Evolvable Hardware; Learning Classifier Systems; Semantic Genetic Programming; Multiobjective Optimization                     |This cluster of papers focuses on the application of genetic programming in machine learning, particularly in the areas of classification, feature selection, symbolic regression, and evolvable hardware. It explores the use of evolutionary algorithms and learning classifier systems to solve complex problems, with an emphasis on multiobjective optimization and semantic genetic programming.                                                                                                                                                                 |https://en.wikipedia.org/wiki/Genetic_programming               |\n",
      "|2072            |Active Learning                 |Active Learning in Machine Learning Research                        |Active Learning; Machine Learning; Semi-supervised Learning; Deep Learning; Gaussian Processes; Image Classification; Text Categorization; Batch Mode; Statistical Guarantees; Human-in-the-loop                                                       |This cluster of papers revolves around the topic of active learning in machine learning research. It covers various aspects such as semi-supervised learning, deep learning, Gaussian processes, image classification, text categorization, batch mode active learning, statistical guarantees, and human-in-the-loop approaches.                                                                                                                                                                                                                                      |https://en.wikipedia.org/wiki/Active_learning_(machine_learning)|\n",
      "|1598            |Internet Traffic Classification |Machine Learning for Internet Traffic Classification                |Machine Learning; Internet Traffic; Classification; Deep Learning; Encrypted Traffic; Network; Anonymity; IoT Devices; Traffic Analysis; E-Voting                                                                                                      |This cluster of papers focuses on the application of machine learning and deep learning techniques for the classification and analysis of internet traffic, including encrypted traffic, network behavior, and IoT device identification. It also covers topics related to anonymity, traffic analysis, and electronic voting.                                                                                                                                                                                                                                         |https://en.wikipedia.org/wiki/Traffic_classification            |\n",
      "|1490            |Hydrological Modeling           |Hydrological Modeling using Machine Learning Methods                |Machine Learning; Hydrology; Forecasting; Artificial Neural Networks; Water Resources; Rainfall-Runoff Modeling; Support Vector Machines; Wavelet Analysis; Model Performance; Groundwater Level Forecasting                                           |This cluster of papers focuses on the application of machine learning methods, such as artificial neural networks, support vector machines, and wavelet analysis, in hydrological modeling and forecasting for water resources management. The papers cover topics including rainfall-runoff modeling, groundwater level forecasting, river flow prediction, and water quality modeling.                                                                                                                                                                               |https://en.wikipedia.org/wiki/Hydrological_modeling             |\n",
      "|1276            |Solar Forecasting               |Machine Learning Methods for Solar Radiation Forecasting            |Solar Radiation; Forecasting; Machine Learning; Artificial Neural Networks; Renewable Energy; Photovoltaic Power; Solar Energy; Grid Integration; Weather Forecasts; GIS-based Site Selection                                                          |This cluster of papers focuses on the application of machine learning methods, particularly artificial neural networks, for forecasting solar radiation and photovoltaic power generation. It covers topics such as solar energy integration, grid-connected PV plant performance prediction, solar position algorithms, and GIS-based site selection for solar farms.                                                                                                                                                                                                 |https://en.wikipedia.org/wiki/Solar_irradiance_forecasting      |\n",
      "|1612            |Machine Learning                |Optimization Methods in Machine Learning                            |Stochastic Gradient Descent; Random Projections; Deep Learning; Convex Optimization; Matrix Decompositions; Approximation Algorithms; Large-Scale Optimization; Neural Networks; Coordinate Descent; Generalization                                    |This cluster of papers focuses on the application of optimization methods in machine learning, particularly in the context of stochastic gradient descent, random projections, deep learning, convex optimization, matrix decompositions, and large-scale optimization. The papers explore various algorithms and techniques for improving the efficiency and effectiveness of machine learning models, with a specific emphasis on neural networks and generalization.                                                                                                |https://en.wikipedia.org/wiki/Machine_learning                  |\n",
      "|2157            |Mineral Prospectivity           |Machine Learning for Mineral Prospectivity Mapping                  |Machine Learning; Mineral Prospectivity; Remote Sensing; Compositional Data Analysis; Geological Mapping; Hyperspectral Imaging; Support Vector Machines; Fractal Modeling; Geochemical Anomalies; Lithological Mapping                                |This cluster of papers focuses on the application of machine learning, remote sensing, and compositional data analysis techniques for mineral prospectivity mapping. It explores the use of advanced technologies such as ASTER and hyperspectral imaging to identify geological features, geochemical anomalies, and hydrothermal alterations associated with mineralization. The cluster also delves into the challenges and opportunities in using support vector machines, fractal modeling, and statistical analysis for predicting undiscovered mineral deposits.|https://en.wikipedia.org/wiki/Mineral_prospecting               |\n",
      "|4064            |Smart Healthcare                |Machine Learning in Smart Healthcare                                |Machine Learning; Smart Cities; Healthcare; IoT; Digital Transformation; Customer Satisfaction; Supply Chain Management; Organizational Performance; Social Media Marketing; Knowledge Management                                                      |This cluster of papers focuses on the application of machine learning, IoT, and digital transformation in smart healthcare, with an emphasis on customer satisfaction, supply chain management, organizational performance, and social media marketing. The research covers topics such as predictive modeling for healthcare outcomes, impact of digital technologies on healthcare services, and leveraging data analytics for improving patient care.                                                                                                               |https://en.wikipedia.org/wiki/Smart_healthcare                  |\n",
      "|1396            |Machine Learning                |Machine Learning in Healthcare and Medicine                         |Machine Learning; Healthcare; Big Data Analytics; Medical Diagnosis; Classification Models; Heart Disease Prediction; Data Mining; Support Vector Machine; Logistic Regression; Feature Selection                                                      |This cluster of papers focuses on the application of machine learning, big data analytics, and data mining techniques in healthcare and medicine. It covers topics such as medical diagnosis, classification models, heart disease prediction, and the use of support vector machines and logistic regression. The cluster emphasizes the potential of machine learning in improving healthcare outcomes through advanced data analysis.                                                                                                                               |https://en.wikipedia.org/wiki/Health_informatics                |\n",
      "|1550            |Multi-label Text Classification |Multi-label Text Classification in Machine Learning                 |Multi-label Learning; Text Classification; Feature Selection; Naive Bayes Classifier; K-nearest Neighbor (KNN); Hierarchical Classification; Machine Learning Algorithms; Document Categorization; Support Vector Machines (SVM); Information Retrieval|This cluster of papers focuses on the application of machine learning algorithms for multi-label text classification, with an emphasis on techniques such as feature selection, Naive Bayes classifier, K-nearest Neighbor (KNN), hierarchical classification, and support vector machines (SVM). The research covers various aspects of document categorization and information retrieval in the context of text mining and natural language processing.                                                                                                              |https://en.wikipedia.org/wiki/Multi-label_classification        |\n",
      "|764             |Privacy-Preserving Data Analysis|Privacy-Preserving Techniques for Data Analysis and Machine Learning|Differential Privacy; Federated Learning; k-Anonymity; Privacy Preservation; Machine Learning; Location Privacy; Data Mining; Anonymization; Secure Computation; Membership Inference Attacks                                                          |This cluster of papers focuses on privacy-preserving techniques for data analysis and machine learning, including topics such as differential privacy, federated learning, k-anonymity, secure computation, and location privacy. The papers explore methods to protect sensitive information while performing data mining, machine learning, and statistical analysis.                                                                                                                                                                                                |https://en.wikipedia.org/wiki/Differential_privacy              |\n",
      "|2814            |Gaussian Processes              |Gaussian Processes in Machine Learning                              |Gaussian Processes; Machine Learning; Variational Inference; Sparse Regression; Bayesian Inference; Deep Learning; Probabilistic Models; Nonparametric Methods; Time Series Modelling; Big Data                                                        |This cluster of papers focuses on the application of Gaussian Processes in machine learning, covering topics such as variational inference, sparse regression, Bayesian inference, deep learning, and probabilistic models. It also explores the use of Gaussian Processes for nonparametric methods, time series modelling, and handling big data.                                                                                                                                                                                                                    |https://en.wikipedia.org/wiki/Gaussian_process                  |\n",
      "+----------------+--------------------------------+--------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_labels.filter(F.col('long_label').contains('Machine Learning')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e490758-57ab-4be1-9d56-e7a0789ab1a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70674439"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_topic_labels = spark.read.parquet(f'{iteration_save_path}topics_data_from_cwts_new') \\\n",
    "    .select(F.col('work_id').cast(LongType()).alias('paper_id'), \n",
    "            F.col('macro_cluster_id').cast(IntegerType()),\n",
    "            F.col('meso_cluster_id').cast(IntegerType()),\n",
    "            F.col('micro_cluster_id').cast(IntegerType())) \\\n",
    "    .filter(F.col('paper_id').isNotNull() & \n",
    "            F.col('macro_cluster_id').isNotNull() & \n",
    "            F.col('meso_cluster_id').isNotNull() & \n",
    "            F.col('micro_cluster_id').isNotNull()) \\\n",
    "    .join(classification_labels, how='inner', on='micro_cluster_id')\n",
    "    \n",
    "new_topic_labels.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be28ecac-648d-450c-bd52-4d1b1553eb74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ef4f83-6314-4a2a-834e-9fde33db9658",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247622936"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_works = spark.read.parquet(f\"{iteration_save_path}new_work_titles\") \\\n",
    "    .dropDuplicates(subset=['paper_id'])\n",
    "\n",
    "works = spark.read.parquet(f\"{base_save_path}static_works\") \\\n",
    "    .select('paper_id','original_title') \\\n",
    "    .union(new_works.select('paper_id','original_title')) \\\n",
    "    .dropDuplicates(subset=['paper_id'])\n",
    "\n",
    "\n",
    "works.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdf64781-94ee-4fef-89a4-553486924363",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|max(paper_id)|\n",
      "+-------------+\n",
      "|   4390089447|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "works.select(F.max('paper_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81df645c-22b4-447f-bf13-5978374fb51c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_abstracts = spark.read.parquet(f\"{iteration_save_path}new_work_abstracts\") \\\n",
    "    .dropDuplicates(subset=['paper_id'])\n",
    "\n",
    "abstracts = spark.read.parquet(f\"{base_save_path}static_abstracts\") \\\n",
    "    .select('paper_id', 'abstract') \\\n",
    "    .union(new_abstracts.select('paper_id','abstract')) \\\n",
    "    .dropDuplicates(subset=['paper_id'])\n",
    "\n",
    "abstracts.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3611a35-76a5-436e-9c91-afc38f8c023a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|max(paper_id)|\n",
      "+-------------+\n",
      "|   4390088933|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abstracts.select(F.max('paper_id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa662736-db57-4f4c-bf3e-80b7c2211b85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "works \\\n",
    "    .join(abstracts, how='left', on='paper_id') \\\n",
    "    .join(new_topic_labels.select('paper_id','micro_cluster_id','short_label','long_label','keywords'), \n",
    "          how='inner', on='paper_id').dropDuplicates(subset=['paper_id']) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{iteration_save_path}language_model/all_training_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bc88e0a-f835-4184-b41d-7a63401ded50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0d98c8a1-f2b6-434b-90aa-8ebbedbf7162",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def name_to_keep_ind(groups):\n",
    "    groups_to_skip = ['HIRAGANA', 'CJK', 'KATAKANA','ARABIC', 'HANGUL', 'THAI','DEVANAGARI','BENGALI',\n",
    "                      'THAANA','GUJARATI','CYRILLIC']\n",
    "    \n",
    "    if any(x in groups_to_skip for x in groups):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def group_non_latin_characters(text):\n",
    "    groups = []\n",
    "    latin_chars = []\n",
    "    text = text.replace(\".\", \"\").replace(\" \", \"\")\n",
    "    for char in text:\n",
    "        try:\n",
    "            script = unicodedata.name(char).split(\" \")[0]\n",
    "            if script == 'LATIN':\n",
    "                latin_chars.append(script)\n",
    "            else:\n",
    "                if script not in groups:\n",
    "                    groups.append(script)\n",
    "        except:\n",
    "            if \"UNK\" not in groups:\n",
    "                groups.append(\"UNK\")\n",
    "    return groups, len(latin_chars)\n",
    "\n",
    "def remove_non_latin_characters(text):\n",
    "    final_char = []\n",
    "    groups_to_skip = ['HIRAGANA', 'CJK', 'KATAKANA','ARABIC', 'HANGUL', 'THAI','DEVANAGARI','BENGALI',\n",
    "                      'THAANA','GUJARATI','CYRILLIC']\n",
    "    for char in text:\n",
    "        try:\n",
    "            script = unicodedata.name(char).split(\" \")[0]\n",
    "            if script not in groups_to_skip:\n",
    "                final_char.append(char)\n",
    "        except:\n",
    "            pass\n",
    "    return \"\".join(final_char)\n",
    "\n",
    "@udf(returnType=IntegerType())\n",
    "def check_for_non_latin_characters(text):\n",
    "    groups, latin_chars = group_non_latin_characters(text)\n",
    "    if name_to_keep_ind(groups) == 1:\n",
    "        return 1\n",
    "    elif latin_chars > 30:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "@udf(returnType=StringType())\n",
    "def clean_title(old_title, keep_title):\n",
    "    if keep_title:\n",
    "        new_title = remove_non_latin_characters(old_title)\n",
    "        if '<' in new_title:\n",
    "            new_title = new_title.replace(\"<i>\", \"\").replace(\"</i>\",\"\")\\\n",
    "                                 .replace(\"<sub>\", \"\").replace(\"</sub>\",\"\") \\\n",
    "                                 .replace(\"<sup>\", \"\").replace(\"</sup>\",\"\") \\\n",
    "                                 .replace(\"<em>\", \"\").replace(\"</em>\",\"\") \\\n",
    "                                 .replace(\"<b>\", \"\").replace(\"</b>\",\"\") \\\n",
    "                                 .replace(\"<I>\", \"\").replace(\"</I>\", \"\") \\\n",
    "                                 .replace(\"<SUB>\", \"\").replace(\"</SUB>\", \"\") \\\n",
    "                                 .replace(\"<scp>\", \"\").replace(\"</scp>\", \"\") \\\n",
    "                                 .replace(\"<font>\", \"\").replace(\"</font>\", \"\") \\\n",
    "                                 .replace(\"<inf>\",\"\").replace(\"</inf>\", \"\") \\\n",
    "                                 .replace(\"<i /> \", \"\") \\\n",
    "                                 .replace(\"<p>\", \"\").replace(\"</p>\",\"\") \\\n",
    "                                 .replace(\"<![CDATA[<B>\", \"\").replace(\"</B>]]>\", \"\") \\\n",
    "                                 .replace(\"<italic>\", \"\").replace(\"</italic>\",\"\")\\\n",
    "                                 .replace(\"<title>\", \"\").replace(\"</title>\", \"\") \\\n",
    "                                 .replace(\"<br>\", \"\").replace(\"</br>\",\"\").replace(\"<br/>\",\"\") \\\n",
    "                                 .replace(\"<B>\", \"\").replace(\"</B>\", \"\") \\\n",
    "                                 .replace(\"<em>\", \"\").replace(\"</em>\", \"\") \\\n",
    "                                 .replace(\"<BR>\", \"\").replace(\"</BR>\", \"\") \\\n",
    "                                 .replace(\"<title>\", \"\").replace(\"</title>\", \"\") \\\n",
    "                                 .replace(\"<strong>\", \"\").replace(\"</strong>\", \"\") \\\n",
    "                                 .replace(\"<formula>\", \"\").replace(\"</formula>\", \"\") \\\n",
    "                                 .replace(\"<roman>\", \"\").replace(\"</roman>\", \"\") \\\n",
    "                                 .replace(\"<SUP>\", \"\").replace(\"</SUP>\", \"\") \\\n",
    "                                 .replace(\"<SSUP>\", \"\").replace(\"</SSUP>\", \"\") \\\n",
    "                                 .replace(\"<sc>\", \"\").replace(\"</sc>\", \"\") \\\n",
    "                                 .replace(\"<subtitle>\", \"\").replace(\"</subtitle>\", \"\") \\\n",
    "                                 .replace(\"<emph/>\", \"\").replace(\"<emph>\", \"\").replace(\"</emph>\", \"\") \\\n",
    "                                 .replace(\"\"\"<p class=\"Body\">\"\"\", \"\") \\\n",
    "                                 .replace(\"<TITLE>\", \"\").replace(\"</TITLE>\", \"\") \\\n",
    "                                 .replace(\"<sub />\", \"\").replace(\"<sub/>\", \"\") \\\n",
    "                                 .replace(\"<mi>\", \"\").replace(\"</mi>\", \"\") \\\n",
    "                                 .replace(\"<bold>\", \"\").replace(\"</bold>\", \"\") \\\n",
    "                                 .replace(\"<mtext>\", \"\").replace(\"</mtext>\", \"\") \\\n",
    "                                 .replace(\"<msub>\", \"\").replace(\"</msub>\", \"\") \\\n",
    "                                 .replace(\"<mrow>\", \"\").replace(\"</mrow>\", \"\") \\\n",
    "                                 .replace(\"</mfenced>\", \"\").replace(\"</math>\", \"\")\n",
    "\n",
    "            if '<mml' in new_title:\n",
    "                all_parts = [x for y in [i.split(\"mml:math>\") for i in new_title.split(\"<mml:math\")] for x in y if x]\n",
    "                final_parts = []\n",
    "                for part in all_parts:\n",
    "                    if re.search(r\"\\>[$%#!^*\\w.,/()+-]*\\<\", part):\n",
    "                        pull_out = re.findall(r\"\\>[$%#!^*\\w.,/()+-]*\\<\", part)\n",
    "                        final_pieces = []\n",
    "                        for piece in pull_out:\n",
    "                            final_pieces.append(piece.replace(\">\", \"\").replace(\"<\", \"\"))\n",
    "                        \n",
    "                        final_parts.append(\" \"+ \"\".join(final_pieces) + \" \")\n",
    "                    else:\n",
    "                        final_parts.append(part)\n",
    "                \n",
    "                new_title = \"\".join(final_parts).strip()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if '<xref' in new_title:\n",
    "                new_title = re.sub(r\"\\<xref[^/]*\\/xref\\>\", \"\", new_title)\n",
    "\n",
    "            if '<inline-formula' in new_title:\n",
    "                new_title = re.sub(r\"\\<inline-formula[^/]*\\/inline-formula\\>\", \"\", new_title)\n",
    "\n",
    "            if '<title' in new_title:\n",
    "                new_title = re.sub(r\"\\<title[^/]*\\/title\\>\", \"\", new_title)\n",
    "\n",
    "            if '<p class=' in new_title:\n",
    "                new_title = re.sub(r\"\\<p class=[^>]*\\>\", \"\", new_title)\n",
    "            \n",
    "            if '<span class=' in new_title:\n",
    "                new_title = re.sub(r\"\\<span class=[^>]*\\>\", \"\", new_title)\n",
    "\n",
    "            if 'mfenced open' in new_title:\n",
    "                new_title = re.sub(r\"\\<mfenced open=[^>]*\\>\", \"\", new_title)\n",
    "            \n",
    "            if 'math xmlns' in new_title:\n",
    "                new_title = re.sub(r\"\\<math xmlns=[^>]*\\>\", \"\", new_title)\n",
    "\n",
    "        if '<' in new_title:\n",
    "            new_title = new_title.replace(\">i<\", \"\").replace(\">/i<\", \"\") \\\n",
    "                                 .replace(\">b<\", \"\").replace(\">/b<\", \"\") \\\n",
    "                                 .replace(\"<inline-formula>\", \"\").replace(\"</inline-formula>\",\"\")\n",
    "\n",
    "        return new_title\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6f68bc-0e8f-4345-808b-60d2a64830f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w2 = Window.partitionBy('micro_cluster_id').orderBy('random_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc785eec-2ef6-48e8-a54c-f1d163695190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70674410"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = spark.read.parquet(f\"{iteration_save_path}language_model/all_training_data/\")\n",
    "\n",
    "train_data.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only taking 1000 samples of each cluster ID in order to limit the amount of training data and also keep the labeled data balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d262fa-6465-41e3-8ad3-3eec02825b77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data \\\n",
    "    .withColumn('random_num', F.rand()) \\\n",
    "    .withColumn('cluster_rank', F.row_number().over(w2)) \\\n",
    "    .filter(F.col('cluster_rank')<=1000) \\\n",
    "    .withColumn('keep_title', check_for_non_latin_characters(F.col('original_title')))\\\n",
    "    .withColumn('new_title', clean_title(F.col('original_title'), F.col('keep_title'))) \\\n",
    "    .select('paper_id','new_title','abstract','micro_cluster_id','short_label','long_label','keywords') \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{iteration_save_path}language_model/all_training_data_subset_new/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5ad52b9-9838-4f64-8684-9ff2cad84139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4521000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(f\"{iteration_save_path}language_model/all_training_data_subset_new/\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c5e092b-9151-4457-b7aa-92602c36cd6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(f\"{iteration_save_path}language_model/all_training_data_subset_new/\") \\\n",
    "    .coalesce(1) \\\n",
    "    .write.mode('overwrite') \\\n",
    "    .parquet(f\"{iteration_save_path}language_model/all_training_data_subset_single_file_new/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f545b058-2dac-45b0-b6cc-f70273f0af9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "concepts_language_model_finetune",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
